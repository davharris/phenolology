---
title: "Prior Analysis"
author: "Susannah Tysor"
date: "20/03/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages}
library(ggplot2)
library(dplyr)
library(bayesplot)
library(rstan)
library(ggpubr)
```

```{r options}
rstan_options(auto_write=TRUE)
options(mc.cores = parallel::detectCores())
```

```{r functions}

# generate samples from a truncated normal distribution. n = how many samples, mean and sd are mean and sd, and min and max are the limits of the distribution/truncation points. possibly my first ever use of while loops. Don't @ me.
rtnorm <- function(n, mean, sd, min, max) {
  x <- rnorm(n, mean=mean, sd=sd)
  x <- x[x >= min & x <= max]
  while(length(x) < n) {
    newx <- rnorm(1, mean=mean, sd=sd)
    while(newx <= min | newx >= max) {
      newx <- rnorm(1, mean=mean, sd=sd)
    }
    x <- c(x, newx)
  }
  length(x)==n
  return(x)
}

# simulate data from an ordinal logistic model and format it as input for stan. input is a list for stan, as is output.
simulate_data <- function(input) {
  simu <- rstan::stan(file='dirichlet prior/covar_sim.stan', iter=1, chains=1, algorithm="Fixed_param", data=input)
  
  simu_params <- rstan::extract(simu) 
  
  input_data_for_model <- list("N" = input$N, "K" = input$K, "x" = input$x, "y" = array(simu_params$y[1,])) 
  return(input_data_for_model)
}

# function to plot modeled parameters with label being a string to label the graph (usually prior and whether groups were included and pardf is the modeled parameter dataframe)
parplotter <- function(label, pardf) {
  cutpoints <- mcmc_areas(pardf, regex_pars="c") +
    geom_vline(xintercept=c)
  beta <- mcmc_areas(pardf, pars="beta") +
    geom_vline(xintercept=beta)
  h <- mcmc_areas(pardf, regex_pars = "h") +
    geom_vline(xintercept=h)
  ggpubr::ggarrange(cutpoints, beta, h, nrow=1, ncol=3, labels=label)
}


# function to calculate the difference between modeled parameters (in the model_params dataframe) and true parameters (globally declared)
posterior_differencer <- function(model_params) {
  c1_diff <- model_params$c.1 - c[1] 
  c2_diff <- model_params$c.2 - c[2]
  h1_diff <- model_params$h1 - h[1]
  h2_diff <- model_params$h2 - h[2] 
  beta_diff <- model_params$beta - beta
  diffframe <- data.frame(c1_diff, c2_diff, h1_diff, h2_diff, beta_diff)
  return(diffframe)
}

# function to plot histograms of differences between true params and modeled params (model_params dataframe). priorgroups is a string that makes a subtitle for the plot
diffplotter <- function(model_params, priorgroups) {
  diffs <- posterior_differencer(model_params)
  cuts <- mcmc_areas(diffs, regex_pars = "c") +
    ggtitle("", subtitle = "differences between modeled and true params")+
  xlim(c(-10,10))
  opars <- mcmc_areas(diffs, pars=c("h1_diff", "h2_diff", "beta_diff")) +
    xlim(c(-1,1))
  ggpubr::ggarrange(cuts, opars, labels=priorgroups)
}


```

## Prior choice

In my original phenology model, I use 

* a gamma distribution for the prior on the cutpoints 
* an exponential distribution distribution for the transition speed/slope $\beta$
* normal distributions for group effects on the slope

Michael Betancourt thinks that an induced dirichlet prior may be a [good choice](https://betanalpha.github.io/assets/case_studies/ordinal_regression.html) for ordinal logistic models.

Normally, priors are bottom up - you choose a distribution for each prior. This is tricky for the cutpoints in an ordinal logistic model because they're defined on an abstract latent space so you can't easily use your domain expertise, but a good prior is incredibly important in these models, because ordered logistic models with covariates are inherently non-identifiable. (Because $beta$s and cutpoints depend on one another.)

Betancourt says that
> To avoid the non-identifiability of the interior cut points from propagating to the posterior distribution we need a principled prior model that can exploit domain expertise to consistently regularize all of the internal cut points at the same time. Regularization of the cut points, however, is subtle given their ordering constraint. Moreover domain expertise is awkward to apply on the abstract latent space where the internal cut points are defined.

# Goals
1) Understand the dynamics of the gamma and induced dirichlet priors and 2) compare them

# Simulate data
First let's simulate some data that's kind of like mine.

We have three potential states `K` and a latent effect/covariate `x`. Data is most likely to be collected around state 2. `x` is always positive.

I'll simulate 2 datasets - one with a fast transition speed ($\beta = 2$) and one with a slow transition speed ($\beta = 0.5$). I want the transitions to occur at the same `x` for both datasets. The halfway transition points `h` will be at `x= 8` and `x=12`. So the cutpoints `c` for the slow transition will be at 4 and 6 and for the fast transition at 16 and 24.


## Simulate datasets

```{r data simulation}
N <- 500
K <- 3

beta_slow <- 0.5
beta_fast <- 2

cutpoints_slow <- c(4,6)
cutpoints_fast <- c(16,24)

h_slow <- cutpoints_slow/beta_slow #half transitions
h_fast <- cutpoints_fast/beta_fast
x <- rtnorm(n=N, mean=mean(h), sd=2, min=0, max=20) #covariate

testthat::test_that("half transition points identical", {
  testthat::expect_equal(cutpoints_slow/beta_slow, cutpoints_fast/beta_fast)
})
```

```{r slow transition simulation}

inputs_for_sim_slow <- list("N" = N, "K" = K, "c" = cutpoints_slow, "beta"=beta_slow, "h" = h, "x" = x)
inputs_for_sim_fast <- list("N" = N, "K" = K, "c" = cutpoints_fast, "beta"=beta_fast, "h" = h, "x" = x)


# simulate data, graph it, and prepare data for model fitting

simdat_slow <- simulate_data(input=inputs_for_sim_slow)
simplot(simdat_slow, label="slow")
simdat_fast <- simulate_data(input=inputs_for_sim_fast)
simplot(simdat_fast, label="fast")
```

## Recapture parameters: Gamma

Now I want to try to recapture parameters.

I need to know how good I have to be at choosing the shape parameter for the gamma prior on the cutpoints to recapture parameters as well. So I'll choose a prior for gamma that roughly centers it between the cutpoints, which I think is a "good" option (though given the long right tail, maybe a lower value would be better). Then halve and double it.

I also want to understand how the exponential prior on beta affects the ability to recapture parameters. I'll try rates of 1,2,3.

So there are 2 simulated datasets (beta=0.5 or beta=2) and I'm going to try to fit them both with 3 rates for the beta's exponential prior and 3 shapes for the cutpoints' gamma prior, a total of 18 model runs.

### Gamma prior with covariate
```{r params for gamma models}

beta_rate <- c(1:3)

shape_slow <- mean(cutpoints_slow)
shape_slow_small <- shape_slow/2
shape_slow_big <- shape_slow*2

slowshapes <- c(shape_slow, shape_slow_small, shape_slow_big)

shape_fast <- mean(cutpoints_fast)
shape_fast_small <- shape_fast/2
shape_fast_big <- shape_fast*2

fastshapes <- c(shape_fast, shape_fast_big, shape_fast_small)

slowframe <- data.frame(beta = beta_slow, shape = slowshapes, beta_rate=beta_rate)
fastframe <- data.frame(beta=beta_fast, shape= fastshapes, beta_rate=beta_rate)
parframe <- rbind(slowframe, fastframe) %>%
  tidyr::expand(tidyr::nesting(beta,shape), beta_rate)

knitr::kable(parframe, caption="beta parameters used to simulate data for a slow (0.5) and fast (2) transion. And shape and rate parameters to be used in the priors on beta and cutpoints to attempt to recover parameters")
```

```{r recapture parameters using a gamma prior}
# append the correct shape (gamma prior on cutpoints) and rate (exponential prior on beta) parameters to the simulated data list and fit a model with a gamma prior
fit_gamma_model <- function(simdat, shape, rate) {
  simdat$shape <- shape
  simdat$rate <- rate
  fitgam <- stan(file='dirichlet prior/gamma/gamma_covar.stan', data=simdat, chains=4)
}

# append the correct shape (gamma prior on cutpoints) and rate (exponential prior on beta) parameters to the simulated data list and fit a model with a gamma prior. simdat is a list of datasets simulated from stan models and pars is a vector of additional parameters. 
fit_gamma_model <- function(simdatlist, pars) {
  #choose whether to use data simulated with a rapid or slow transition
  if (pars$beta == beta_slow) {
    simdat <- simdatlist$simdat_slow
  }
  if (pars$beta == beta_fast) {
    simdat <- simdatlist$simdat_fast
  }
  #extract parameters for prior distribtuions
  simdat$shape <- pars$shape
  simdat$rate <- pars$beta_rate
  
  #fit the model
  fitgam <- stan(file='dirichlet prior/gamma/gamma_covar.stan', data=simdat, chains=4)
  return(fitgam)
}


library(parallel)
parlist <- split(parframe, seq(nrow(parframe)))
datlist <- list(simdat_slow=simdat_slow, simdat_fast=simdat_fast)

# make a cluster, leaving 1 free so your computer doesn't fall over when you check emails
no_cores <- detectCores() - 1
cl <- makeCluster(no_cores)

# export the stuff you need to run on the cluster
clusterExport(cl, c("fit_gamma_model", "parlist", "beta_fast", "beta_slow", "datlist"))
clusterEvalQ(cl, c(library(rstan), library(StanHeaders)))
gammafits <- parLapply(cl, parlist[1:3], function(x) {fit_gamma_model(simdatlist <- datlist, pars=x)})

stopCluster(c1)




paramsgam1 <- data.frame(rstan::extract(fit1)) 

sum1 <- summary(paramsgam)

pgam <- parplot("gamma", paramsgam, cutpoints=cutpoints_slow, beta=beta_slow, h=h_slow)
pgam

diffplotter(model_params=paramsgam, priorgroups="gamma prior, no groups")
```

Gamma prior recaptures parameters relatively well here and there are no obvious model fitting problems. However, it pretty much always overestimates beta and cutpoints (while accurately identifying the inflection points). It's not especially sensitive to the prior on $\beta$ either. An $\mathrm{exponential}$ prior with rates of 1,2,3, and 6 on $\beta$ performed similarly.


### Dirichlet prior with covariate
What about a dirichlet prior?

```{r dirichlet recapture}
fitdir <- stan(file='dirichlet prior/dirichlet/dirichlet_covar.stan', data=input_data_for_model, chains=4)

paramsdir <- data.frame(rstan::extract(fitdir))
summary(paramsdir)

pdir <- parplotter("dirichlet", paramsdir)
pdir

```

The model with an induced dirichlet prior anchored at 0 had divergences, treedepth issues, and other warnings. It didn't fit well. I think it's because it was estimating some negative cutpoints and going off into weird territory with beta.

Anchoring the dirichlet at 3 didn't seem to improve things.


```{r compare gamma and dirichlet}


ddir <- diffplotter(paramsdir, priorgroups="dirichlet prior, no groups")
dgam <- diffplotter(paramsgam, priorgroups = "gamma prior, no groups")
ggarrange(dgam, ddir, ncol=1)

ggarrange(pgam, pdir, ncol=1)

```


## Simulate more data - uniform covariate?
What if your data is, unlike mine, collected more uniformly across the covariate range?

```{r data simulation}
N = 500
G = 7
K = 3

c <- c(3,7) # cutpoints
beta = 0.5
h <- c/beta
x <- runif(N, min=0, max=20) #covariate
hist(x)
```

```{r simulate covar}
input_data_for_simulation <- list("N" = N, "K"=3, "x" = x, "c" = c, "beta" = beta)

simu <- stan(file='dirichlet prior/covar_sim.stan', iter=1, chains=1,
             algorithm="Fixed_param", data=input_data_for_simulation)

simu_params <- rstan::extract(simu)

input_data_for_model <- list("N" = N, "K" = K, "x" = x, "y" = array(simu_params$y[1,]))

inputdf <- data.frame(input_data_for_model)

ggplot(inputdf, aes(x=x, colour=as.factor(y))) +
    stat_ecdf() +
    geom_vline(xintercept=h)

plot(input_data_for_model$x, input_data_for_model$y)
title("Simulated data with cutpoints")
abline(v=h)

```

### Induced dirichlet prior with uniformly distributed covariate
```{r dirichlet recapture}
fitdir <- stan(file='dirichlet prior/dirichlet/dirichlet_covar.stan', data=input_data_for_model, chains=4)

paramsdir <- data.frame(rstan::extract(fitdir))
summary(paramsdir)

pdir <- parplotter("dirichlet", paramsdir)
pdir

diffplotter(model_params=paramsdir, priorgroups="induced dirichlet prior, no groups")

```

When the covariate is more evenly distributed, the dirichlet does a great job! 

## Simulate more data - $\beta=2$

Does the induced dirichlet do better with sharper transitions - if beta gets increased to 2?


```{r data simulation}
N = 500
G = 7
K = 3

c <- c(3,7) # cutpoints
beta = 2
h <- c/beta
x <- rtnorm(n=N, mean=mean(h), sd=1.5*mean(h), min=0, max=20) #covariate
hist(x)
```

```{r simulate covar}
input_data_for_simulation <- list("N" = N, "K"=3, "x" = x, "c" = c, "beta" = beta)

simu <- stan(file='dirichlet prior/covar_sim.stan', iter=1, chains=1,
             algorithm="Fixed_param", data=input_data_for_simulation)

simu_params <- rstan::extract(simu)

input_data_for_model <- list("N" = N, "K" = K, "x" = x, "y" = array(simu_params$y[1,]))

inputdf <- data.frame(input_data_for_model)

ggplot(inputdf, aes(x=x, colour=as.factor(y))) +
    stat_ecdf() +
    geom_vline(xintercept=h)

plot(input_data_for_model$x, input_data_for_model$y)
title("Simulated data with cutpoints")
abline(v=h)

```


### Dirichlet prior with covariate

With a more rapid transition between states, the induced dirichlet is pretty good!

```{r dirichlet recapture}
fitdir <- stan(file='dirichlet prior/dirichlet/dirichlet_covar.stan', data=input_data_for_model, chains=4)

paramsdir <- data.frame(rstan::extract(fitdir))
summary(paramsdir)

pdir <- parplotter("dirichlet", paramsdir)
pdir

diffplotter(model_params=paramsdir, priorgroups="induced dirichlet prior, no groups")

```

Will the gamma still work well under these conditions?
### Gamma prior with covariate
```{r gamma recapture}
fitgam <- stan(file='dirichlet prior/gamma/gamma_covar.stan', data=input_data_for_model, chains=4)

paramsgam <- data.frame(rstan::extract(fitgam)) 

summary(paramsgam)

pgam <- parplotter("gamma", paramsgam)
pgam

diffplotter(model_params=paramsgam, priorgroups="gamma prior, no groups")
```

Yep! Gamma outperforms dirichlet.

```{r compare gamma and dirichlet}


ddir <- diffplotter(paramsdir, priorgroups="dirichlet prior, no groups")
dgam <- diffplotter(paramsgam, priorgroups = "gamma prior, no groups")
ggarrange(dgam, ddir, ncol=1)

ggarrange(pgam, pdir, ncol=1)

```


*tl;dr* Gamma outperforms dirichlet when observations are bunched up around a particular state. 


# Covariate and a group

What if our data has groups that have different effects? 

## Group effects just a little smaller than $\beta$ 
What if the effects are around the same size as beta and beta is relatively small (which makes the model harder to fit)?

Let's simulate 7 groups with `N=500` observations for each group. Effects for the groups are simulated from a normal distribution with `mean=0` and `sd=0.25`. They're deviations from the population mean.

```{r group simulation}

N = 500*7
G = 7
K = 3

# parameters
c <- c(3,7) # cutpoints
beta = 0.5

# simulate individual group effects
gbeta_mu <- 0
gbeta_sd <- 0.25

gbeta_vec <- rnorm(G, mean=gbeta_mu, sd=gbeta_sd) # group effects
gbeta <- sort(sample(gbeta_vec, size=N, replace=TRUE)) # assign a group effect to every observation
gid <- as.numeric(as.factor(gbeta)) # label the groups
groupeffects <- data.frame(gbeta, gid) %>%
  unique()

h1 <- c[1]/(beta+groupeffects$gbeta)
h2 <- c[2]/(beta+groupeffects$gbeta)
x <- rtnorm(n=N, mean=mean(c(h1,h2)), sd=mean(h)/4, min=0, max=20) #covariate centered around transitions
hist(x)

input_data_for_simulation <- list("N" = N, "K"=K, "G"=G, "x" = x, "c" = c, "beta"=beta, "gbeta"= gbeta)

simu <- stan(file='dirichlet prior/covar_group_sim.stan', iter=1, chains=1,
             algorithm="Fixed_param", data=input_data_for_simulation)

simu_params <- rstan::extract(simu)

input_data_for_model <- list("N" = N, "K" = K, "G"=G, "x" = x, "GID"= gid, "y" = array(simu_params$y[1,]))

table(input_data_for_model$y)
plot(input_data_for_model$x, input_data_for_model$y)

groupdata <- data.frame(x=x, y=input_data_for_model$y, GID=gid)
ggplot(groupdata, aes(x=x, color=as.factor(y))) +
    stat_ecdf() +
    facet_wrap("GID")
```

### Gamma recapture 
Can a model with the gamma prior recapture the parameters used to simulate the data?

The group effect prior ~ $\mathrm{normal}(0,sigma_{group})$ with $sigma_{group} ~ \mathrm{exponential}(4)$

```{r gamma recapture group}
fitgam <- stan(file='dirichlet prior/gamma/gamma_covar_group.stan', data=input_data_for_model, chains=4, control = list(adapt_delta=0.95, max_treedepth=11), iter=5000)

paramsgamg <- data.frame(rstan::extract(fitgam)) 

summary(paramsgamg)

# function to make a graph for a group of parameter values comparing them to the "true" value


parplot <- function(label, pardf) {
  cutpointsp <- mcmc_areas(pardf, regex_pars="c") +
    geom_vline(xintercept=c)
  betap <- mcmc_areas(pardf, pars="beta") +
    geom_vline(xintercept=beta)
  betagp <- mcmc_areas(pardf, regex_pars="betag") +
    geom_vline(xintercept = groupeffects$gbeta)
  hp <- mcmc_areas(pardf, regex_pars = "fstart") +
    geom_vline(xintercept=h1)
  ggpubr::ggarrange(cutpointsp, betap, betagp, hp, labels=label)
}

pgam <- parplot("gamma, beta=0.5", paramsgamg)
pgam

diffplotter(model_params=paramsgam, priorgroups="gamma prior, no groups")
```

### These notes are for data simulated around base population h (with group effects subtracted out), not individual group h's.
There was a divergent transition, treedepth warning, and ESS warnings with the default adapt delta, so I increased it to 0.9. Still problems. 

Trying 0.95. No more divergences, but 1 transition after warmup exceeded max treedept and still some bulk ESS problems. $\beta$ parameters are all overestimated.

Keeping adapt_delta at 0.95 and increasing max treedepth to 11 leaves only ESS problems. Estimates are not great - all beta parameters are overestimated and there's no differentiation between them. Not ideal.

Attempting an increase in iterations - that kills all the Stan errors! Unfortunately, $\beta$, $\beta_g$s, and $sigma_group$ are still really overestimated and broad (just bad!). And none of the $\beta_g$s are negative.

On the one hand, this is not great from a model perspective. OTOH, these are pretty unrealistic group effects - ones that shift fstart quite a lot.

### These notes are for data simulated around an h based on the entire population, including groups

Tail ESS errors with iter at 4000, increased to 5000. No more warnings. But how does estimation go? Still badly. Fails to distinguish between groups, overestimates beta, underestimates cutpoints.


## Group effects about 1/4 the size of beta.
Is it the relative size of $beta$ and $betag$s that's the problem or the absolute size of $betag$? Let's bump up $beta$ to the easier to fit 2 (more rapid state transitions) and see how our model does.

```{r group simulation big beta}

N = 500*7
G = 7
K = 3

# parameters
c <- c(5,10) # cutpoints
beta = 1

# simulate individual group effects
gbeta_mu <- 0
gbeta_sd <- 0.25

gbeta_vec <- rnorm(G, mean=gbeta_mu, sd=gbeta_sd) # group effects
gbeta <- sort(sample(gbeta_vec, size=N, replace=TRUE)) # assign a group effect to every observation
gid <- as.numeric(as.factor(gbeta)) # label the groups
groupeffects <- data.frame(gbeta, gid) %>%
  unique()

h1 <- c[1]/(beta+groupeffects$gbeta)
h2 <- c[2]/(beta+groupeffects$gbeta)
x <- rtnorm(n=N, mean=mean(c(h1,h2)), sd=mean(h), min=0, max=20) #covariate
hist(x, breaks=30)
     

input_data_for_simulation <- list("N" = N, "K"=K, "G"=G, "x" = x, "c" = c, "beta"=beta, "gbeta"= gbeta)

simu <- stan(file='dirichlet prior/covar_group_sim.stan', iter=1, chains=1,
             algorithm="Fixed_param", data=input_data_for_simulation)

simu_params <- rstan::extract(simu)

input_data_for_model <- list("N" = N, "K" = K, "G"=G, "x" = x, "GID"= gid, "y" = array(simu_params$y[1,]))

table(input_data_for_model$y)
plot(input_data_for_model$x, input_data_for_model$y)

groupdata <- data.frame(x=x, y=input_data_for_model$y, GID=gid)
ggplot(groupdata, aes(x=x, color=as.factor(y))) +
    stat_ecdf() +
    facet_wrap("GID")
```


```{r gamma recapture group big beta}
fitgam <- stan(file='dirichlet prior/gamma/gamma_covar_group.stan', data=input_data_for_model, chains=4, control = list(adapt_delta=0.95, max_treedepth=11), iter=4000)

fitgam <- stan(file='dirichlet prior/gamma/gamma_covar_group.stan', data=input_data_for_model, chains=4)

paramsgamg <- data.frame(rstan::extract(fitgam)) 

summary(paramsgamg)

# function to make a graph for a group of parameter values comparing them to the "true" value


pgam <- parplot("gamma, beta=2", paramsgamg)
pgam

```

Model doesn't give any obvious warnings, but  it overestimates cutpoints, underestimates beta, and dramatically overestimates betags.

If I change the priors from `cutpoints ~ gamma(10,1)` to `gamma(5,1)` and reduce the sigma_group prior to `exp(5)` from `exp(4)`, it does the same.

If I bump up cutpoints so that the covariate distribution isn't bumping up against 0, beta is underestimated and group betas really overestimated.

If I strongly constrain the sigma_group prior to `exp(10)`, it doesn't change anything.

If I use a uniform distribution for the covariate, beta is underestimated, group betas are still really overestimated (maybe even worse!) and not differentiated.

Why are all the group effect betas positive and undifferentiated? Does the model need more data? This run took... a really long time. 44109.6 seconds. And it had warnings... And the fit was even worse.

```
Warning messages:
1: There were 393 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 11. See
http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded 
2: Examine the pairs() plot to diagnose sampling problems
```

Can the group effects be recovered if I explicity specify the sd as 0.25 in the model? No it cannot. That's depressing.

What if beta is bigger compared to group effects - like 5? Even more disastrous. Beta is SO overestimated.

What if I drop beta to 1? Still garbage with betas all so overestimated.

I think I might be simulating my data kind of poorly - or maybe this model isn't good about simulating this data? Or maybe beta should be bigger? I think the transitions between states occurs faster in my data.

I could also try a really big beta - like 10.